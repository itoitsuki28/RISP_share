{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71fef61c-37bc-44be-9c2e-48aaec19b464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm==3.3.2 (from -r requirements.txt (line 1))\n",
      "  Using cached lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
      "Collecting holidays (from -r requirements.txt (line 2))\n",
      "  Obtaining dependency information for holidays from https://files.pythonhosted.org/packages/5d/8c/bad7c11afc8969834728c0678bbf8f3ec5dba4c4ac7f5ad8cf91d63e865f/holidays-0.40-py3-none-any.whl.metadata\n",
      "  Using cached holidays-0.40-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pandas_datareader (from -r requirements.txt (line 3))\n",
      "  Using cached pandas_datareader-0.10.0-py3-none-any.whl (109 kB)\n",
      "Collecting tslearn (from -r requirements.txt (line 4))\n",
      "  Obtaining dependency information for tslearn from https://files.pythonhosted.org/packages/97/22/8dba9a7149d51fe0b6163a5a6b7efc315ab3c097cb6b0d1fc649a03f2722/tslearn-0.6.3-py3-none-any.whl.metadata\n",
      "  Using cached tslearn-0.6.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting category_encoders (from -r requirements.txt (line 5))\n",
      "  Obtaining dependency information for category_encoders from https://files.pythonhosted.org/packages/7f/e5/79a62e5c9c9ddbfa9ff5222240d408c1eeea4e38741a0dc8343edc7ef1ec/category_encoders-2.6.3-py2.py3-none-any.whl.metadata\n",
      "  Using cached category_encoders-2.6.3-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from lightgbm==3.3.2->-r requirements.txt (line 1)) (0.41.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lightgbm==3.3.2->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm==3.3.2->-r requirements.txt (line 1)) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.10/site-packages (from lightgbm==3.3.2->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from holidays->-r requirements.txt (line 2)) (2.8.2)\n",
      "Collecting lxml (from pandas_datareader->-r requirements.txt (line 3))\n",
      "  Obtaining dependency information for lxml from https://files.pythonhosted.org/packages/b6/7f/544a50ffb6cffa3fc1060e5e1c6e2b369fe87bf9c582f73b8c99948ab675/lxml-5.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached lxml-5.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.10/site-packages (from pandas_datareader->-r requirements.txt (line 3)) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pandas_datareader->-r requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from tslearn->-r requirements.txt (line 4)) (0.58.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from tslearn->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders->-r requirements.txt (line 5)) (0.14.0)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from category_encoders->-r requirements.txt (line 5)) (0.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.23->pandas_datareader->-r requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.23->pandas_datareader->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.1->category_encoders->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pandas_datareader->-r requirements.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pandas_datareader->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pandas_datareader->-r requirements.txt (line 3)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pandas_datareader->-r requirements.txt (line 3)) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.2->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.9.0->category_encoders->-r requirements.txt (line 5)) (23.1)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->tslearn->-r requirements.txt (line 4)) (0.41.0)\n",
      "Using cached holidays-0.40-py3-none-any.whl (889 kB)\n",
      "Using cached tslearn-0.6.3-py3-none-any.whl (374 kB)\n",
      "Using cached category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
      "Using cached lxml-5.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (8.0 MB)\n",
      "Installing collected packages: lxml, holidays, tslearn, pandas_datareader, lightgbm, category_encoders\n",
      "Successfully installed category_encoders-2.6.3 holidays-0.40 lightgbm-3.3.2 lxml-5.0.0 pandas_datareader-0.10.0 tslearn-0.6.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import holidays\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, f1_score, silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import category_encoders as ce\n",
    "\n",
    "import lightgbm as lgb\n",
    "from tslearn.clustering import TimeSeriesKMeans, KShape\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.metrics import cdist_dtw\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "\n",
    "import warnings\n",
    "import requests\n",
    "import os\n",
    "import tqdm\n",
    "from scipy.spatial import distance, KDTree\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e40eb5-c6a8-43f6-b4e0-22601573c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "target = pd.read_csv('target.csv')\n",
    "\n",
    "def category(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "def zero_process(df):\n",
    "    temp_target = target.copy()\n",
    "    temp_target.drop(columns=['product_ProductName','mean_used_num'], inplace=True)\n",
    "    temp_target['date'] = ''\n",
    "    concatenated_df = pd.DataFrame()\n",
    "    for date in df['date'].unique():\n",
    "        temp_target['date'] = date\n",
    "        concatenated_df = pd.concat([concatenated_df, temp_target], axis=0)\n",
    "    df = pd.concat([df, concatenated_df]).sort_values(by='date')\n",
    "    df.fillna({'lineItem_UsageAccountId': 0, 'sum_num_machine': 0}, inplace=True)\n",
    "    df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def get_test(df):\n",
    "    df = zero_process(df)\n",
    "    df.drop(['lineItem_UsageAccountId','sum_num_machine'],axis=1,inplace=True)\n",
    "    df = df.drop_duplicates(subset=['date', 'customer','product_region','product_operatingSystem','product_instanceType'])\n",
    "    df = df.merge(target, left_on=['customer', 'product_region', 'product_operatingSystem','product_instanceType'], right_on=['customer', 'product_region', 'product_operatingSystem','product_instanceType'])\n",
    "    df.drop(['product_ProductName','mean_used_num'],axis=1,inplace=True)    \n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    \n",
    "    country_holidays = holidays.CountryHoliday('JP')\n",
    "    df['weekday'] = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        current_date = pd.to_datetime(row['date'])\n",
    "        df.at[index, 'weekday'] = current_date.strftime(\"%A\")\n",
    "    df=category(df)\n",
    "    #df.drop('date',axis=1,inplace=True)\n",
    "    return df\n",
    "    \n",
    "def get_train(df):\n",
    "    df = zero_process(df)\n",
    "    df.drop('lineItem_UsageAccountId', axis=1, inplace=True)\n",
    "    df['total_sum_num_machine'] = df.groupby(['date', 'customer','product_region','product_operatingSystem','product_instanceType'])['sum_num_machine'].transform('sum')\n",
    "    df = df.drop_duplicates(subset=['date', 'customer','product_region','product_operatingSystem','product_instanceType'])\n",
    "    df = df.merge(target, left_on=['customer', 'product_region', 'product_operatingSystem','product_instanceType'], right_on=['customer', 'product_region', 'product_operatingSystem','product_instanceType'])\n",
    "    df.drop(['product_ProductName','mean_used_num','sum_num_machine'],axis=1,inplace=True)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    \n",
    "    country_holidays = holidays.CountryHoliday('JP')\n",
    "    df['weekday'] = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        current_date = pd.to_datetime(row['date'])\n",
    "        df.at[index, 'weekday'] = current_date.weekday()\n",
    "    df=category(df)\n",
    "    df.drop('date',axis=1,inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = get_train(df)\n",
    "test = get_test(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a1287-ea9d-4ad8-b439-8728216e093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['total_sum_num_machine']\n",
    "X = df.drop('total_sum_num_machine',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc83bd0-88e2-49db-9e89-de3841a245db",
   "metadata": {},
   "source": [
    "## サイクリカルエンコーディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add44b1-7a62-4b0e-b9a8-9d634a487111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cyclical_encoding(df):\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['weekday'] = df['weekday'].astype('int32')\n",
    "    df['sin_day_of_week'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "    df['cos_day_of_week'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
    "\n",
    "\n",
    "    condition_31_days = df['month'].isin([1, 3, 5, 7, 8, 10, 12])\n",
    "    condition_30_days = df['month'].isin([4, 6, 9, 11])\n",
    "    condition_28_days = ~condition_31_days & ~condition_30_days\n",
    "\n",
    "    max_days = np.select([condition_31_days, condition_30_days, condition_28_days], [31, 30, 28])\n",
    "\n",
    "    df['sin_day'] = np.sin(2 * np.pi * df['day'] / max_days)\n",
    "    df['cos_day'] = np.cos(2 * np.pi * df['day'] / max_days)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9fd9a-ce87-491b-838a-c21ed2416bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cyclical_encoding(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db0e09-6b58-4041-8ad8-6689b675d476",
   "metadata": {},
   "source": [
    "## 単純ターゲットエンコーディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ff819-bfbe-4b8b-89e8-4999f103f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def simple_target_enc(col):\n",
    "    #skf = StratifiedKFold(n_splits=5)\n",
    "    kf = KFold(n_splits=5)\n",
    "    encoded_features = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X,y):\n",
    "        X_train_, X_valid_ = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_ = y.iloc[train_idx]\n",
    "\n",
    "        target_encoder = ce.TargetEncoder()\n",
    "        target_encoder.fit(X_train_[col], y_train_)\n",
    "\n",
    "        X_valid_[f'target_{col}'] = target_encoder.transform(X_valid_[col])\n",
    "        encoded_features.append(X_valid_)\n",
    "\n",
    "\n",
    "    encoded_df = pd.concat(encoded_features).sort_index()\n",
    "    df_with_encoded = pd.merge(X, encoded_df[[f'target_{col}']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "    target_encoder = ce.TargetEncoder()\n",
    "    target_encoder.fit(df_with_encoded[col], y)\n",
    "\n",
    "    test[f'target_{col}'] = target_encoder.transform(test[col])\n",
    "\n",
    "    df_with_encoded.drop(col,axis=1,inplace=True)\n",
    "    test.drop(col,axis=1,inplace=True)\n",
    "    \n",
    "    return df_with_encoded, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e2a16-2b33-4f38-8c0c-b3c3cbff8d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,test = simple_target_enc('product_instanceType')\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27fd678-7f64-49cf-8866-918396a6d7fb",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
