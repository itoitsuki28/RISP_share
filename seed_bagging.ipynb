{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cf77874-39d2-47fd-afd6-96fd1d064c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm==3.3.2 (from -r requirements.txt (line 1))\n",
      "  Using cached lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
      "Collecting holidays (from -r requirements.txt (line 2))\n",
      "  Obtaining dependency information for holidays from https://files.pythonhosted.org/packages/5d/8c/bad7c11afc8969834728c0678bbf8f3ec5dba4c4ac7f5ad8cf91d63e865f/holidays-0.40-py3-none-any.whl.metadata\n",
      "  Using cached holidays-0.40-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pandas_datareader (from -r requirements.txt (line 3))\n",
      "  Using cached pandas_datareader-0.10.0-py3-none-any.whl (109 kB)\n",
      "Collecting tslearn (from -r requirements.txt (line 4))\n",
      "  Obtaining dependency information for tslearn from https://files.pythonhosted.org/packages/97/22/8dba9a7149d51fe0b6163a5a6b7efc315ab3c097cb6b0d1fc649a03f2722/tslearn-0.6.3-py3-none-any.whl.metadata\n",
      "  Using cached tslearn-0.6.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting category_encoders (from -r requirements.txt (line 5))\n",
      "  Obtaining dependency information for category_encoders from https://files.pythonhosted.org/packages/7f/e5/79a62e5c9c9ddbfa9ff5222240d408c1eeea4e38741a0dc8343edc7ef1ec/category_encoders-2.6.3-py2.py3-none-any.whl.metadata\n",
      "  Using cached category_encoders-2.6.3-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from lightgbm==3.3.2->-r requirements.txt (line 1)) (0.41.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lightgbm==3.3.2->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm==3.3.2->-r requirements.txt (line 1)) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.10/site-packages (from lightgbm==3.3.2->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from holidays->-r requirements.txt (line 2)) (2.8.2)\n",
      "Collecting lxml (from pandas_datareader->-r requirements.txt (line 3))\n",
      "  Obtaining dependency information for lxml from https://files.pythonhosted.org/packages/b6/7f/544a50ffb6cffa3fc1060e5e1c6e2b369fe87bf9c582f73b8c99948ab675/lxml-5.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached lxml-5.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/conda/lib/python3.10/site-packages (from pandas_datareader->-r requirements.txt (line 3)) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pandas_datareader->-r requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from tslearn->-r requirements.txt (line 4)) (0.58.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from tslearn->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from category_encoders->-r requirements.txt (line 5)) (0.14.0)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from category_encoders->-r requirements.txt (line 5)) (0.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.23->pandas_datareader->-r requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.23->pandas_datareader->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.1->category_encoders->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pandas_datareader->-r requirements.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pandas_datareader->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pandas_datareader->-r requirements.txt (line 3)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pandas_datareader->-r requirements.txt (line 3)) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.2->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.9.0->category_encoders->-r requirements.txt (line 5)) (23.1)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->tslearn->-r requirements.txt (line 4)) (0.41.0)\n",
      "Using cached holidays-0.40-py3-none-any.whl (889 kB)\n",
      "Using cached tslearn-0.6.3-py3-none-any.whl (374 kB)\n",
      "Using cached category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
      "Using cached lxml-5.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (8.0 MB)\n",
      "Installing collected packages: lxml, holidays, tslearn, pandas_datareader, lightgbm, category_encoders\n",
      "Successfully installed category_encoders-2.6.3 holidays-0.40 lightgbm-3.3.2 lxml-5.0.0 pandas_datareader-0.10.0 tslearn-0.6.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import holidays\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, f1_score, silhouette_score, calinski_harabasz_score, davies_bouldin_score,mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import category_encoders as ce\n",
    "\n",
    "import lightgbm as lgb\n",
    "from tslearn.clustering import TimeSeriesKMeans, KShape\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.metrics import cdist_dtw\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "\n",
    "import warnings\n",
    "import requests\n",
    "import os\n",
    "import tqdm\n",
    "from scipy.spatial import distance, KDTree\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e5fc16-3284-4be9-b5ea-c80d6da48388",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "target = pd.read_csv('target.csv')\n",
    "\n",
    "def cyclical_encoding(df):\n",
    "    df['sin_month'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['weekday'] = df['weekday'].astype('int32')\n",
    "    df['sin_day_of_week'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "    df['cos_day_of_week'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
    "\n",
    "\n",
    "    condition_31_days = df['month'].isin([1, 3, 5, 7, 8, 10, 12])\n",
    "    condition_30_days = df['month'].isin([4, 6, 9, 11])\n",
    "    condition_28_days = ~condition_31_days & ~condition_30_days\n",
    "\n",
    "    max_days = np.select([condition_31_days, condition_30_days, condition_28_days], [31, 30, 28])\n",
    "\n",
    "    df['sin_day'] = np.sin(2 * np.pi * df['day'] / max_days)\n",
    "    df['cos_day'] = np.cos(2 * np.pi * df['day'] / max_days)\n",
    "\n",
    "    return df\n",
    "\n",
    "def simple_target_enc(col):\n",
    "    #skf = StratifiedKFold(n_splits=5)\n",
    "    kf = KFold(n_splits=5)\n",
    "    encoded_features = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X,y):\n",
    "        X_train_, X_valid_ = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train_ = y.iloc[train_idx]\n",
    "\n",
    "        target_encoder = ce.TargetEncoder()\n",
    "        target_encoder.fit(X_train_[col], y_train_)\n",
    "\n",
    "        X_valid_[f'target_{col}'] = target_encoder.transform(X_valid_[col])\n",
    "        encoded_features.append(X_valid_)\n",
    "\n",
    "\n",
    "    encoded_df = pd.concat(encoded_features).sort_index()\n",
    "    df_with_encoded = pd.merge(X, encoded_df[[f'target_{col}']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "    target_encoder = ce.TargetEncoder()\n",
    "    target_encoder.fit(df_with_encoded[col], y)\n",
    "\n",
    "    test[f'target_{col}'] = target_encoder.transform(test[col])\n",
    "\n",
    "    df_with_encoded.drop(col,axis=1,inplace=True)\n",
    "    test.drop(col,axis=1,inplace=True)\n",
    "    \n",
    "    return df_with_encoded, test\n",
    "\n",
    "def category(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "def zero_process(df):\n",
    "    temp_target = target.copy()\n",
    "    temp_target.drop(columns=['product_ProductName','mean_used_num'], inplace=True)\n",
    "    temp_target['date'] = ''\n",
    "    concatenated_df = pd.DataFrame()\n",
    "    for date in df['date'].unique():\n",
    "        temp_target['date'] = date\n",
    "        concatenated_df = pd.concat([concatenated_df, temp_target], axis=0)\n",
    "    df = pd.concat([df, concatenated_df]).sort_values(by='date')\n",
    "    df.fillna({'lineItem_UsageAccountId': 0, 'sum_num_machine': 0}, inplace=True)\n",
    "    df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def get_test(df):\n",
    "    df = zero_process(df)\n",
    "    df.drop(['lineItem_UsageAccountId','sum_num_machine'],axis=1,inplace=True)\n",
    "    df = df.drop_duplicates(subset=['date', 'customer','product_region','product_operatingSystem','product_instanceType'])\n",
    "    df = df.merge(target, left_on=['customer', 'product_region', 'product_operatingSystem','product_instanceType'], right_on=['customer', 'product_region', 'product_operatingSystem','product_instanceType'])\n",
    "    df.drop(['product_ProductName','mean_used_num'],axis=1,inplace=True)    \n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    \n",
    "    country_holidays = holidays.CountryHoliday('JP')\n",
    "    df['weekday'] = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        current_date = pd.to_datetime(row['date'])\n",
    "        df.at[index, 'weekday'] = current_date.weekday()\n",
    "        \n",
    "    df=category(df)\n",
    "    df.drop('date',axis=1,inplace=True)\n",
    "    df = cyclical_encoding(df)\n",
    "    return df\n",
    "    \n",
    "def get_train(df):\n",
    "    df = zero_process(df)\n",
    "    df.drop('lineItem_UsageAccountId', axis=1, inplace=True)\n",
    "    df['total_sum_num_machine'] = df.groupby(['date', 'customer','product_region','product_operatingSystem','product_instanceType'])['sum_num_machine'].transform('sum')\n",
    "    df = df.drop_duplicates(subset=['date', 'customer','product_region','product_operatingSystem','product_instanceType'])\n",
    "    df = df.merge(target, left_on=['customer', 'product_region', 'product_operatingSystem','product_instanceType'], right_on=['customer', 'product_region', 'product_operatingSystem','product_instanceType'])\n",
    "    df.drop(['product_ProductName','mean_used_num','sum_num_machine'],axis=1,inplace=True)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    \n",
    "    country_holidays = holidays.CountryHoliday('JP')\n",
    "    df['weekday'] = None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        current_date = pd.to_datetime(row['date'])\n",
    "        df.at[index, 'weekday'] = current_date.weekday()\n",
    "    df=category(df)\n",
    "    df.drop('date',axis=1,inplace=True)\n",
    "    df = cyclical_encoding(df)\n",
    "    return df\n",
    "\n",
    "df = get_train(df)\n",
    "test = get_test(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf5bdfb-6560-4167-848e-b78fa116218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['total_sum_num_machine']\n",
    "X = df.drop('total_sum_num_machine',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "247f14ab-fab7-4fb1-9837-eee91aae5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['product_operatingSystem','product_region','product_instanceType']\n",
    "for col in columns:\n",
    "    X,test = simple_target_enc(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c55f1248-fc40-4ff5-9520-675e9cfb00db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_state : 28\n",
      "average_mse : 488462.23386066774\n",
      "average_mae : 255.875131632568\n",
      " \n",
      "random_state : 2\n",
      "average_mse : 488440.4320132747\n",
      "average_mae : 255.83532793461754\n",
      " \n",
      "random_state : 4\n",
      "average_mse : 488453.48471510183\n",
      "average_mae : 255.83483263629037\n",
      " \n",
      "random_state : 15\n",
      "average_mse : 488445.6913954068\n",
      "average_mae : 255.84771656756644\n",
      " \n",
      "random_state : 100\n",
      "average_mse : 488447.9396685388\n",
      "average_mae : 255.85012684491963\n",
      " \n",
      "random_state : 11\n",
      "average_mse : 488442.7712461798\n",
      "average_mae : 255.87463707218808\n",
      " \n",
      "random_state : 13\n",
      "average_mse : 488446.37401679583\n",
      "average_mae : 255.8721302883946\n",
      " \n",
      "random_state : 30\n",
      "average_mse : 488436.60552178195\n",
      "average_mae : 255.8350160845194\n",
      " \n",
      "random_state : 34\n",
      "average_mse : 488439.02056014934\n",
      "average_mae : 255.83371595466022\n",
      " \n",
      "random_state : 77\n",
      "average_mse : 488438.4945258576\n",
      "average_mae : 255.83354578994903\n",
      " \n",
      "Average Bagging complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "import statistics\n",
    "\n",
    "final_predictions = []\n",
    "random_state = [28, 2, 4, 15, 100, 11, 13, 30, 34, 77]\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for seed in random_state:\n",
    "    print(f'random_state : {seed}')\n",
    "    average_mse = []\n",
    "    average_mae = []\n",
    "    predictions = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        lgb_params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mape',\n",
    "            'random_state': seed,\n",
    "            'n_estimators': 1000,\n",
    "            \"verbosity\": -1,\n",
    "            \"importance_type\": \"gain\"\n",
    "        }\n",
    "\n",
    "        model = lgb.LGBMRegressor(**lgb_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        average_mse.append(mse)\n",
    "        average_mae.append(mae)\n",
    "\n",
    "    average_mse = statistics.mean(average_mse)\n",
    "    average_mae = statistics.mean(average_mae)\n",
    "    print(f'average_mse : {average_mse}')\n",
    "    print(f'average_mae : {average_mae}')\n",
    "    print(' ')\n",
    "\n",
    "    predictions.append(model.predict(test))\n",
    "    del train_idx,val_idx,average_mse,average_mae\n",
    "\n",
    "final_predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "print('Average Bagging complete.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
